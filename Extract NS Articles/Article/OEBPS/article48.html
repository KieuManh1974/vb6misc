<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="application/xhtml+xml; charset=UTF-8" /><title>Article 48</title>
<link href="stylesheet.css" type="text/css" rel="stylesheet" />
<link rel="stylesheet" type="application/vnd.adobe-page-template+xml" href="page-template.xpgt"/>
</head>
<body>
<h1>Opinion > The Big Idea</h1>
<h2>
		
		
			Out of control: How to live in an unfathomable world
		
		</h2>
<p><i>We need to accept that the interactions of technology, society and nature are now beyond our understanding</i></p>
<p>SOON after the combination of an earthquake and the resulting tsunami destroyed the Fukushima nuclear power plant in Japan earlier this year, opposing sides began to mobilise their standard arguments to reach foregone conclusions. Obviously, the disaster showed that nuclear power was insanely risky and should be abandoned. Equally obviously, new reactors will be designed to avoid such risks and will be crucial for meeting the planet's growing energy and environmental demands.</p>
<p>These opposing positions are predictable, but they are also incoherent, unintelligible and entirely unhelpful in navigating the complexities of our technological age. In <i>The Techno-Human Condition</i>, we argue that this unintelligibility comes from a failure to distinguish between the different levels of technological complexity, and a resulting confusion about the limits of rational action in the world.</p>
<p>At its simplest level, technology is designed to perform a particular function whose key attributes are integral to the technology itself. Complicated as it may be, a nuclear reactor is an engineered system that delivers baseload electrical energy with high reliability. The technology is the function. In our book, we call this Level I.</p>
<p>At Level II, a technology is part of a complex network. The reactor, for example, is linked to the electricity grid upon which people depend for their well-being. But the grid in turn is linked to other complex networks, for manufacturing, transportation, information and communication, and so on.</p>
<p>At Level III, complexity becomes pervasive - integrated in ways that can never be fully understood, with an array of human, built and natural subsystems creating the adaptive systems which increasingly characterise Earth. Here, nuclear power intersects with the motion of tectonic plates, as well as with cultural and social forces such as fear of global climate change and demands for rising standards of living.</p>
<p>This three-level classification makes explicit what is otherwise often concealed. Level I considerations, and to a lesser extent Level II concerns, dominate our thinking. We create, understand and experience technology at these levels, we evaluate the viability, desirability and risk of technology at these levels, and the complexity does not overwhelm us.</p>
<p>Level III effects are no less real, but tend to get swept under the rug as "unintended consequences" that come from beyond the Level I and II world we consciously seek to create. Yet Level III effects, the complexities of the anthropogenic Earth, are an inescapable consequence of the technological evolution of human society.</p>
<p>Humanity's commitment to technological change is a commitment to the creation of more uncertainty, contingency and incomprehensibility. Indeed, you are surrounded by and are at this very moment being profoundly changed by Level III systems whose implications you cannot fathom. With input from tablet computers, cameraphones and walls of dancing video, and with much of your memory outsourced to Google and your social relations to Facebook, you now embody the accelerating charge of the Five Horsemen of converging technology - nanotechnology, biotechnology, robotics, information and communication technology, and applied cognitive science - whose cumulative potency will transform the human-Earth system in ways that are impossible to predict.</p>
<p>As with nuclear power, current debates and policy responses tend to polarise. Some see salvation in the Five Horsemen, others a juggernaut that drains humanity of dignity and free will. These dialogues are, frankly, not only simple-minded but increasingly dysfunctional. Having created a technological world, we desperately need to work out how to better understand it, and how to live in it rationally, responsibly and ethically.</p>
<p>The techno-pessimist critique overlooks the fact that technologies are popular because they work, often performing crucial functions. The vaccinations I am given as a child alter my immune system so I can expect to live to old age; my phone tells me the time, takes pictures and presents augmented reality commentary on London; a plane flies me from New York to London with remarkable safety.</p>
<p>The techno-optimist critique, however, overlooks the often problematic, systemic impacts of technology: vaccines change expectations about survival and contribute to global and regional demographic challenges; cellphone technology dramatically reduces privacy and subtly transforms cognitive patterns; planes can be vectors of disease.</p>
<p>Our science and engineering, our ways of reasoning and our ethical frameworks, even our notions of individuality and free will, all assume a Level I world in which we are able to map discernible cause-and-effect relations onto our actions and our plans. But we have bad news: these tools, the defining aspects of human identity and ambition, apply only weakly at Level II, and when it comes to Level III they can be more harmful than helpful.</p>
<p>This is a scary, potentially destabilising, idea for a culture weaned on Enlightenment notions of rationality, but it's where, ironically enough, our rationality has taken us, via the incredible potency and complexity of the technological systems that rationality has enabled us to create. However you may feel about the kinds of Level I technologies that the Five Horseman may soon have to offer - about brain-machine interfaces that make us smarter, about intelligent cyborg insect robot swarms that infiltrate our enemies, about organ farms to keep us from dying, and about factory meat to feed us - there is little question that the cumulative result will be a new level of complexity that we cannot yet begin to comprehend.</p>
<p>We are not the "knowledge society"; that's Level I talk. We are in fact an ignorance society, continually creating more and more ignorance as we busily expand the complexity of the anthropogenic Earth. But our ignorance is not a "problem" with a "solution": it is inherent in the techno-human condition.</p>
<p>The world we are creating thus demands a transition from our almost paranoid societal obsession with Level I certainty and coherence to acceptance that Level III uncertainties and contradictions are the essence of the world we have already made. The question now is how to enable rational and ethical behaviour in a world too complex for applied rationality, how to make our ignorance an opportunity for continual learning and adjustment.</p>
<p>This necessary evolution does not demand radical changes in human behaviour and institutions, but the opposite: a courageous realisation that the condition we are always trying to escape - of ignorance and disagreement about the consequences of our actions - is in fact the source of the imagination and agility necessary to act wisely in the Level III world. The basic civil commitments of democratic societies, designed to mediate uncertainty and disagreement, offer the foundation for our institutions to evolve in the necessary direction.</p>
<p>But they will need help. We have to become a lot smarter in moving ourselves and our institutions of learning and innovation, of political and economic decision-making, out of their Level I playrooms. This transition will require us to increase the diversity of world views involved in creating and assessing our technological activities. It asks us to create more richly imagined futures, seeded with more potential choices, so that we have improved opportunities to learn from and respond to the choices we are making.</p>
<p>Fortunately, there are plenty of examples to build on, from farming communities that know how to hedge against uncertain climate conditions, to aircraft carriers that build continual learning and improvement into their operations, and innovative corporations that use scenario planning to survive market unpredictability.</p>
<p>Humans may be the new design space, but we also remain the holders of moral responsibility for the anthropogenic world. The challenges of accelerating technological change and rapidly increasing complexity are inherent in our age. Thus, our final suggestion is that to participate ethically, rationally and responsibly in the world we are creating together, we must accept fundamental cognitive dissonance as integral to the techno-human condition. What we believe most deeply, we must distrust most strongly.</p>
<h2 class="greybox">Profile</h2>
<p><b>Braden R. Allenby</b> is Lincoln Professor of Engineering and Ethics and professor of civil and environmental engineering at Arizona State University, Tempe. <b>Daniel Sarewitz</b> is professor of science and society and co-director of the Consortium for Science, Policy and Outcomes at ASU. Their book <i>The Techno-Human Condition</i> is published by MIT Press</p>
</body>
</html>
